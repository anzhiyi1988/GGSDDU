### 1.4.1 连接和认证参数

Connection and Authentication Parameters

#### 1.4.1.1 Connection Parameters

gp_connection_send_timeout
gp_vmem_idle_resource_timeout
listen_addresses
max_connections
max_prepared_transactions
superuser_reserved_connections
tcp_keepalives_count
tcp_keepalives_idle
tcp_keepalives_interval
unix_socket_directory
unix_socket_group
unix_socket_permissions

#### 1.4.1.2 Security and Authentication Parameters

authentication_timeout
db_user_namespace
krb_caseins_users
krb_server_keyfile
krb_srvname
password_encryption
password_hash_algorithm
ssl
ssl_ciphers

### 1.4.2 系统资源消耗参数

System Resource Consumption Parameters

#### 1.4.2.1内存消耗参数

Memory Consumption Parameters

These parameters control system memory usage. You can adjust gp_vmem_protect_limit to avoid
running out of memory at the segment hosts during query processing.

控制系统内存的使用，可以调整`gp_vmem_protect_limit`避免查询时segment服务器内存不足。

##### gp_vmem_idle_resource_timeout

If a database session is idle for longer than the time specified, the session will free system resources (such
as shared memory), but remain connected to the database. This allows more concurrent connections to
the database at one time.

如果一个会话空闲时间超过指定的时间，数据库会释放会话使用的资源(如共享内存)，但是会保持连接，此机制保证多用户并行连接数据库（不用资源，只有连接）

| Value Range                                | Default | Set Classifications  |
| ------------------------------------------ | ------- | -------------------- |
| Any valid time expression(number and unit) | 18s     | master system reload |

##### gp_vmem_protect_limit

Sets the amount of memory (in number of MBs) that all postgres processes of an active segment instance
can consume. If a query causes this limit to be exceeded, memory will not be allocated and the query
will fail. Note that this is a local parameter and must be set for every segment in the system (primary and mirrors). When setting the parameter value, specify only the numeric value. For example, to specify
4096MB, use the value 4096. Do not add the units MB to the value.

设置active segment实例上所有postgres进程可消耗的内存总量（单位是MBs），如果一个查询超出此限制，内存不会再分配，这个查询也会失败。注意这个参数是本地化参数，所有segment点都需要设置。设置时指明数值即可，如设置4096MB，值设置为4096，不需要加MB指定单位。

To prevent over-allocation of memory, these calculations can estimate a safe gp_vmem_protect_limit
value.

为了防止过渡分配内存，以下计算可以估算gp_vmem_protect_limit的值。



First calculate the value gp_vmem. This is the Greenplum Database memory available on a host

首先计算gp_vmem的值，这个值是GPD在机器上可使用的内存：

```
gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.7
```

where SWAP is the host swap space and RAM is the RAM on the host in GB.



Next, calculate the max_acting_primary_segments. This is the maximum number of primary segments
that can be running on a host when mirror segments are activated due to a failure. With mirrors
arranged in a 4-host block with 8 primary segments per host, for example, a single segment host failure
would activate two or three mirror segments on each remaining host in the failed host's block. The
max_acting_primary_segments value for this configuration is 11 (8 primary segments plus 3 mirrors
activated on failure).

接下来计算max_acting_primary_segments，这个值是由于故障而激活mirror segments加上该主机上可运行的primary segments数的和的最大值。例如四个主机，每个主机8个p segments，当一个主机故障，则其他三个主机的m segments要启动，那么每个机器应该启动2到3个mirror，所以此时每个主机的最大运行p segments就是11个（p8+m3）， 所以max_acting_primary_segments值为11。



This is the calculation for gp_vmem_protect_limit. The value should be converted to MB.

接下来计算gp_vmem_protect_limit的值，并转换为MB即可

```
gp_vmem_protect_limit = gp_vmem / acting_primary_segments
```



For scenarios where a large number of workfiles are generated, this is the calculation for gp_vmem that
accounts for the workfiles：

如果有大量的文件生成，则计算时需要考虑文件数量：

```
gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM - (300KB * total_#_workfiles))) / 1.7
```

For information about monitoring and managing workfile usage, see the Greenplum Database
Administrator Guide.

有关文件使用信息产看admin guide。



Based on the gp_vmem value you can calculate the value for the vm.overcommit_ratio operating system
kernel parameter. This parameter is set when you configure each Greenplum Database host.

通过gp_vmem 的值可以计算操作系统内核参数 vm.overcommit_ratio的值

```
vm.overcommit_ratio = (RAM - (0.026 * gp_vmem)) / RAM
```

> Note: The default value for the kernel parameter vm.overcommit_ratio in Red Hat Enterprise
> Linux is 50.
>
> 注意，系统Red Hat Enterprise
> Linux默认的vm.overcommit_ratio值为50
>
> For information about the kernel parameter, see the Greenplum Database Installation Guide.



| Value Range | Default | Set Classifications  |
| ----------- | ------- | -------------------- |
| integer     | 8192    | local system restart |





##### gp_vmem_protect_segworker_cache_limit

If a query executor process consumes more than this configured amount, then the process will not be
cached for use in subsequent queries after the process completes. Systems with lots of connections or idle
processes may want to reduce this number to free more memory on the segments. Note that this is a local
parameter and must be set for every segment.

如果一个运行中的查询进程消耗的资源超过此值，那么这个查询结束后不会被缓存用于后续查询。如果系统的连接数和空闲进程比较多，那么系统会希望减少这个值。注意这个值是本地参数，所有段都需要配置。

| Value Range         | Default | Set Classifications  |
| ------------------- | ------- | -------------------- |
| number of megabytes | 500     | local system restart |



##### gp_workfile_limit_files_per_query

Sets the maximum number of temporary spill files (also known as workfiles) allowed per query per
segment. Spill files are created when executing a query that requires more memory than it is allocated. The
current query is terminated when the limit is exceeded.

设置每个段上的每个查询的最大临时溢出文件（工作文件）数量，当一个执行查询创建spill files时需要比分配时更多的内存，如果文件数超过限制，当前查询会终止。

Set the value to 0 (zero) to allow an unlimited number of spill files

设置为0时表示无限的文件数量

|Value Range| Default |Set Classifications|
|----|----|----|
|integer| 100000| master session reload|

##### gp_workfile_limit_per_query

Sets the maximum disk size an individual query is allowed to use for creating temporary spill files at each
segment. The default value is 0, which means a limit is not enforced.

每个段上的单个查询创建工作文件时允许使用的最大磁盘大小，默认为0，表示不强制限制。

|Value Range |Default |Set Classifications|
|----|----|----|
|kilobytes |0 |master session reload|

##### gp_workfile_limit_per_segment

Sets the maximum total disk size that all running queries are allowed to use for creating temporary spill
files at each segment. The default value is 0, which means a limit is not enforced.

每个段上所有运行中的查询创建工作文件时，允许使用的最大磁盘大小，默认为0，表示不强制限制。

|Value Range|Default|Set Classifications|
|----|----|----|
|kilobytes|0|local system restart|



##### max_stack_depth

Specifies the maximum safe depth of the server's execution stack. The ideal setting for this parameter
is the actual stack size limit enforced by the kernel (as set by ulimit -s or local equivalent), less a safety
margin of a megabyte or so. Setting the parameter higher than the actual kernel limit will mean that a
runaway recursive function can crash an individual backend process

指定服务器运行堆栈的最大安全深度，此参数的理想值是内核的堆栈大小限制减去一兆左右的安全量（内核的堆栈大小限制由ulimit -s或本地等效项设置），如果此参数比内核大，那么一个失控的递归会到导致后台进程崩溃。

|Value Range|Default|Set Classifications|
|----|----|----|
|number of kilobytes|2MB|local system restart|



##### shared_buffers

Sets the amount of memory a Greenplum Database segment instance uses for shared memory buffers.
This setting must be at least 128KB and at least 16KB times max_connections.

设置每个段的实例使用的共享内存缓存区的大小，此参数最少128KB 并且至少为（16KB * max_connections）。



Each Greenplum Database segment instance calculates and attempts to allocate certain amount of shared
memory based on the segment configuration. The value of shared_buffers is significant portion of this
shared memory calculation, but is not all it. When setting shared_buffers, the values for the operating
system parameters SHMMAX or SHMALL might also need to be adjusted.

每个段实例基于配置参数计算并尝试分配一定数量的共享内存，shared_buffers的计算是共享内存计算的重要一部分但不是全部。当设置shared_buffers参数时，操作系统的参数SHMMAX 或者SHMALL 有可能也需要调整。



The operating system parameter SHMMAX specifies maximum size of a single shared memory allocation.
The value of SHMMAX must be greater than this value:

操作系统SHMMAX 参数指定最大单个共享内存分配值，这个值必须比下面的值大：

```
shared_buffers + other_seg_shmem
```

The value of other_seg_shmem is the portion the Greenplum Database shared memory calculation that
is not accounted for by the shared_buffers value. The other_seg_shmem value will vary based on the
segment configuration. With the default Greenplum Database parameter values, the value for other_seg_shmem is approximately
111MB for Greenplum Database segments and approximately 79MB for the Greenplum Database master.

other_seg_shmem 的值是GPD共享内存计算的一部分，不被shared_buffers 占用，这个值是基于段配置的，GPD的segment节点的此值大约为111MB，master节点大约为79MB。



The operating system parameter SHMALL specifies the maximum amount of shared memory on the host.
The value of SHMALL must be greater than this value:

操作系统参数指定服务器上的最大共享内存，这个值必须比下面的值大：

```
(num_instances_per_host * ( shared_buffers + other_seg_shmem )) + other_app_shared_mem
```

The value of other_app_shared_mem is the amount of shared memory that is used by other applications
and processes on the host.

other_app_shared_mem 的值是操作系统上其他应用和进程的共享内存的值。

When shared memory allocation errors occur, possible ways to resolve shared memory allocation issues
are to increase SHMMAX or SHMALL, or decrease shared_buffers or max_connections.

当共享内存分配错误，可以通过增加SHMMAX或者SHMALL的值解决，也可以减少 shared_buffers或者max_connections的值。

See the Greenplum Database Installation Guide for information about the Greenplum Database values for
the parameters SHMMAX and SHMALL.

|Value Range|Default|Set Classifications|
|----|----|----|
|integer > 16K * max_connections|125MB|local system  restart|

##### temp_buffers

Sets the maximum number of temporary buffers used by each database session. These are session-local
buffers used only for access to temporary tables. The setting can be changed within individual sessions,
but only up until the first use of temporary tables within a session. The cost of setting a large value in
sessions that do not actually need a lot of temporary buffers is only a buffer descriptor, or about 64 bytes,
per increment. However if a buffer is actually used, an additional 8192 bytes will be consumed.

设置每一个数据库连接session的临时缓冲区大小，这些临时缓冲区只限访问临时表。每个session可以单独更改此值，但是直到session中第一次使用临时表为止。在会话中设置一个很大的值其实不会生效，因为一个session如不不需要临时缓冲区，那么这个值只是一个描述符，占用约64字节，然而如果真的到使用时，会增加8192字节的消耗。

|Value Range|Default|Set Classifications|
|----|----|----|
|integer|1024|master session reload|

#### Free Space Map Parameters

These parameters control the sizing of the free space map, which contains expired rows. Use VACUUM to
reclaim the free space map disk space.

设置过期数据可用空间的大小，使用VACUUM释放空间。

##### max_fsm_pages

Sets the maximum number of disk pages for which free space will be tracked in the shared freespace map. Six bytes of shared memory are consumed for each page slot. If this value is increased,
shared_buffers and the kernel parameter SHMMAX might also need to be increased. Also, parameters that
depend on shared memory such as max_connections might need to be changed.

在共享内存的freespace map需要跟踪可用空间，所以设置最大磁盘页面数，如果调大此值，shared_buffers 和SHMMAX可能也需要增加，另外涉及共享内存参数例如max_connections 可能也需要调整

|Value Range|Default|Set Classifications|
|--|--|--|
|integer > 16 * max_fsm_relations|200000|local system restart|



##### max_fsm_relations

Sets the maximum number of relations for which free space will be tracked in the shared memory freespace
map. Should be set to a value larger than the total number of:

在共享内存的freespace map需要跟踪可用空间，所以设置relations的最大数，这个数要大于下列数：

```
tables + indexes + system tables
```

It costs about 60 bytes of memory for each relation per segment instance. It is better to allow some room
for overhead and set too high rather than too low.

每个segment实例的每个relation消耗60 bytes，设置时最好预留空间，设置高比设置低好。

|Value Range |Default| Set Classifications|
|--|--|--|
|integer| 1000| local system restart |



#### OS Resource Parameters

##### max_files_per_process

Sets the maximum number of simultaneously open files allowed to each server subprocess. If the kernel
is enforcing a safe per-process limit, you don't need to worry about this setting. Some platforms such as
BSD, the kernel will allow individual processes to open many more files than the system can really support.

控制一个进程可以同时打开多少个文件的，如果内核有强制要求，则忽略这个设置，有些平台例如BSD，他的内核允许打开的文件数超过系统真正支持的数量。

|Value Range| Default| Set Classifications|
|--|--|--|
|integer |1000 |local system restart|

##### shared_preload_libraries 

这个不翻译



#### Cost-Based Vacuum Delay Parameters

#### Transaction ID Management Parameters

### 1.4.3 查询优化参数

#### GPORCA Configuration Parameters

#### Query Plan Operator Control Parameters

The following parameters control the types of plan operations the legacy query optimizer can use. Enable
or disable plan operations to force the legacy query optimizer to choose a different plan. This is useful for
testing and comparing query performance using different plan types.

#### Legacy Query Optimizer Costing Parameters

#### Database Statistics Sampling Parameters

#### Sort Operator Configuration Parameters

#### Aggregate Operator Configuration Parameters

#### Join Operator Configuration Parameters

#### Other Legacy Query Optimizer Configuration Parameters

#### Other Legacy Query Optimizer Configuration Parameter

### 1.4.4 错误报告和日志参数

### 1.4.5 系统监控参数

### 1.4.6 运行时统计信息收集参数

These parameters control the server statistics collection feature. When statistics collection is enabled, you
can access the statistics data using the pg_stat and pg_statio family of system catalog views.

开启后，您可以通过pg_stat和pg_statio访问系统目录视图的统计数据

### 1.4.7 自动统计信息收集参数

Automatic Statistics Collection Parameters

When automatic statistics collection is enabled, you can run ANALYZE automatically in the same transaction
as an INSERT, UPDATE, DELETE, COPY or CREATE TABLE...AS SELECT statement when a certain threshold
of rows is affected (on_change), or when a newly generated table has no statistics (on_no_stats).
To enable this feature, set the following server configuration parameters in your Greenplum master
postgresql.conf file and restart Greenplum Database:

当开启后，在执行INSERT, UPDATE, DELETE, COPY 或者 CREATE TABLE...AS SELECT 语句时，或者一个新的没有统计信息的表更新时，如果数据的改变达到了预定的阀值，则ANALYZE自动执行。以下参数需要在master的postgresql.conf中设置，设置后需要重启数据库系统

```
• gp_autostats_mode
• gp_autostats_mode_in_functions
• log_autostats
• gp_autostats_on_change_threshold -- 阀值
```

Warning: Depending on the specific nature of your database operations, automatic statistics
collection can have a negative performance impact. Carefully evaluate whether the default setting
of on_no_stats is appropriate for your system.

警告：对性能有影响。

### 1.4.8 客户端默认连接参数

### 1.4.9 锁管理参数

### 1.4.10 工作负载管理

Workload Management Parameters

The following configuration parameters configure the Greenplum Database workload management feature
(resource queues), query prioritization, memory utilization and concurrency control.

以下参数配置数据库的工作负载（资源队列）、查询优化、内存利用率和并发控制。

### 1.4.11 外部表参数

### 1.4.12 数据库表参数

### 1.4.13 Database and Tablespace/Filespace Parameters

### 1.4.14 旧版本兼容性参数

Past PostgreSQL Version Compatibility Parameters
The following parameters provide compatibility with older PostgreSQL versions. You do not need to
change these parameters in Greenplum Database

这些参数不需要修改

### 1.4.15 数组配置参数

The parameters in this topic control the configuration of the Greenplum Database array and its
components: segments, master, distributed transaction manager, master mirror, and interconnect.

### 1.4.16 GP master和segment mirroring参数

### 1.4.17 扩展参数