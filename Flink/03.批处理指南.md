# 转换函数说明

## Map

一对一转换，对每行的某列数据进行处理，但是本来是多列，结果却变成一列了，操蛋，这个函数没学明白。

```java
data.map(new MapFunction<String, Integer>() {
  public Integer map(String value) { return Integer.parseInt(value); }
});
```



## FlapMap

把多行数据经过转换后返回，这个期间你可以改变每一个行的内容，比如某行某列的值为某个值时舍弃，比如每行拆分成多行等。

原文：Takes one element and produces zero, one, or more elements. 一行可变零到多行。

```java
data.flatMap(new FlatMapFunction<String, String>() {
  public void flatMap(String value, Collector<String> out) {
    for (String s : value.split(" ")) {
      out.collect(s);
    }
  }
});
```



## MapPartition

如果要处理的数据用setParallelism分区了，则这个函数在处理时，是按照区的划分处理，每次只处理一个区数据，**区之间是并行还是串行，这个我不知道**。

```
data.mapPartition(new MapPartitionFunction<String, Long>() {
  public void mapPartition(Iterable<String> values, Collector<Long> out) {
    long c = 0;
    for (String s : values) {
      c++;
    }
    out.collect(c);
  }
});
```



## Filter

对结果集进行过滤

```java
data.filter(new FilterFunction<Integer>() {
  public boolean filter(Integer value) { return value > 1000; }
});
```



## Reduce

把这个集合做统一的合并，合并的方式需要自己定义，最后就生成一条数据

```java
data.reduce(new ReduceFunction<Integer> {
  // a b  代表集合中的一条数据
  public Integer reduce(Integer a, Integer b) { return a + b; }
});


data.reduce(new ReduceFunction<obj3> {
  // a b  代表集合中的一条数据
  public Integer reduce(obj1 a, obj2 b) { 
      // 一顿操作 a和b，然后在返回新的obj，obj123都是同一个类的示例。
      return new obj(); 
  }
});
```

## ReduceGroup

## Aggregate

## Distinct

## Join

## OuterJoin

## Cogroup

## cross

## union

合并两个集合

## Rebalance

## Hash-Partition

## Range-Partition

## Custom-Partitioning

## Sort Partition

## First-n



# 数据源获取方式说明

File-based:

- `readTextFile(path)` / `TextInputFormat` - 逐行读取，字符串返回。
- `readTextFileWithValue(path)` / `TextValueInputFormat` - 逐行读取，放回StringValues类型，StringValues是可变字符串。
- `readCsvFile(path)` / `CsvInputFormat` - 处理有字段分隔符的文件，返回tuples or POJOs类型的dataset。
- `readFileOfPrimitives(path, Class)` / `PrimitiveInputFormat` - Parses files of new-line (or another char sequence) delimited primitive data types such as `String` or `Integer`.
- `readFileOfPrimitives(path, delimiter, Class)` / `PrimitiveInputFormat` - Parses files of new-line (or another char sequence) delimited primitive data types such as `String` or `Integer` using the given delimiter.

Collection-based:

- `fromCollection(Collection)` - Creates a data set from a Java.util.Collection. All elements in the collection must be of the same type.
- `fromCollection(Iterator, Class)` - Creates a data set from an iterator. The class specifies the data type of the elements returned by the iterator.
- `fromElements(T ...)` - Creates a data set from the given sequence of objects. All objects must be of the same type.
- `fromParallelCollection(SplittableIterator, Class)` - Creates a data set from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
- `generateSequence(from, to)` - Generates the sequence of numbers in the given interval, in parallel.

Generic:

- `readFile(inputFormat, path)` / `FileInputFormat` - Accepts a file input format.
- `createInput(inputFormat)` / `InputFormat` - Accepts a generic input format.

```java
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// read text file from local files system
DataSet<String> localLines = env.readTextFile("file:///path/to/my/textfile");

// read text file from an HDFS running at nnHost:nnPort
DataSet<String> hdfsLines = env.readTextFile("hdfs://nnHost:nnPort/path/to/my/textfile");

// read a CSV file with three fields
DataSet<Tuple3<Integer, String, Double>> csvInput = env.readCsvFile("hdfs:///the/CSV/file")
	                       .types(Integer.class, String.class, Double.class);

// read a CSV file with five fields, taking only two of them
DataSet<Tuple2<String, Double>> csvInput = env.readCsvFile("hdfs:///the/CSV/file")
                               .includeFields("10010")  // take the first and the fourth field
	                       .types(String.class, Double.class);

// read a CSV file with three fields into a POJO (Person.class) with corresponding fields
DataSet<Person>> csvInput = env.readCsvFile("hdfs:///the/CSV/file")
                         .pojoType(Person.class, "name", "age", "zipcode");

// read a file from the specified path of type SequenceFileInputFormat
DataSet<Tuple2<IntWritable, Text>> tuples =
 env.createInput(HadoopInputs.readSequenceFile(IntWritable.class, Text.class, "hdfs://nnHost:nnPort/path/to/file"));

// creates a set from some given elements
DataSet<String> value = env.fromElements("Foo", "bar", "foobar", "fubar");

// generate a number sequence
DataSet<Long> numbers = env.generateSequence(1, 10000000);

// Read data from a relational database using the JDBC input format
DataSet<Tuple2<String, Integer> dbData =
    env.createInput(
      JDBCInputFormat.buildJDBCInputFormat()
                     .setDrivername("org.apache.derby.jdbc.EmbeddedDriver")
                     .setDBUrl("jdbc:derby:memory:persons")
                     .setQuery("select name, age from persons")
                     .setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO))
                     .finish()
    );

// Note: Flink's program compiler needs to infer the data types of the data items which are returned
// by an InputFormat. If this information cannot be automatically inferred, it is necessary to
// manually provide the type information as shown in the examples above.
```



# 数据输出阶段

Data sinks consume DataSets and are used to store or return them. Data sink operations are described using an [OutputFormat](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/OutputFormat.java). Flink comes with a variety of built-in output formats that are encapsulated behind operations on the DataSet:

- `writeAsText()` / `TextOutputFormat` - Writes elements line-wise as Strings. The Strings are obtained by calling the *toString()*method of each element.
- `writeAsFormattedText()` / `TextOutputFormat` - Write elements line-wise as Strings. The Strings are obtained by calling a user-defined *format()* method for each element.
- `writeAsCsv(...)` / `CsvOutputFormat` - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the *toString()* method of the objects.
- `print()` / `printToErr()` / `print(String msg)` / `printToErr(String msg)` - Prints the *toString()* value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to *print*. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.
- `write()` / `FileOutputFormat` - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.
- `output()`/ `OutputFormat` - Most generic output method, for data sinks that are not file based (such as storing the result in a **database**).

A DataSet can be input to multiple operations. Programs can write or print a data set and at the same time run additional transformations on them.

```java
// text data
DataSet<String> textData = // [...]

// write DataSet to a file on the local file system
textData.writeAsText("file:///my/result/on/localFS");

// write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort
textData.writeAsText("hdfs://nnHost:nnPort/my/result/on/localFS");

// write DataSet to a file and overwrite the file if it exists
textData.writeAsText("file:///my/result/on/localFS", WriteMode.OVERWRITE);

// tuples as lines with pipe as the separator "a|b|c"
DataSet<Tuple3<String, Integer, Double>> values = // [...]
values.writeAsCsv("file:///path/to/the/result/file", "\n", "|");

// this writes tuples in the text formatting "(a, b, c)", rather than as CSV lines
values.writeAsText("file:///path/to/the/result/file");

// this writes values as strings using a user-defined TextFormatter object
values.writeAsFormattedText("file:///path/to/the/result/file",
    new TextFormatter<Tuple2<Integer, Integer>>() {
        public String format (Tuple2<Integer, Integer> value) {
            return value.f1 + " - " + value.f0;
        }
    });
```

数据库输出

```java
DataSet<Tuple3<String, Integer, Double>> myResult = [...]

// write Tuple DataSet to a relational database
myResult.output(
    // build and configure OutputFormat
    JDBCOutputFormat.buildJDBCOutputFormat()
                    .setDrivername("org.apache.derby.jdbc.EmbeddedDriver")
                    .setDBUrl("jdbc:derby:memory:persons")
                    .setQuery("insert into persons (name, age, height) values (?,?,?)")
                    .finish()
    );
```

输出时支持本地排序，所谓的本地就是在单个节点上落地排序，全局的排序不支持，所以是sortpartition，按照分区排序，代码如下：

```java
DataSet<Tuple3<Integer, String, Double>> tData = // [...]
DataSet<Tuple2<BookPojo, Double>> pData = // [...]
DataSet<String> sData = // [...]

// sort output on String field in ascending order 按照第二列升序
tData.sortPartition(1, Order.ASCENDING).print();

// sort output on Double field in descending and Integer field in ascending order
// 按照第三列降序再按照第一列升序
tData.sortPartition(2, Order.DESCENDING).sortPartition(0, Order.ASCENDING).print();

// sort output on the "author" field of nested BookPojo in descending order
pData.sortPartition("f0.author", Order.DESCENDING).writeAsText(...);

// sort output on the full tuple in ascending order
tData.sortPartition("*", Order.ASCENDING).writeAsCsv(...);

// sort atomic type (String) output in descending order
sData.sortPartition("*", Order.DESCENDING).writeAsText(...);
```



# 迭代操作（no no no）

flink中更有两种迭代操作BulkIteration和DeltaIteration，**这个迭代是什么意思，我也没有弄明白。**后续在整理吧

## Bulk Iterations



```java
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// Create initial IterativeDataSet 
// 初始迭代数据集
IterativeDataSet<Integer> initial = env.fromElements(0).iterate(10000);

// 定义迭代方式
DataSet<Integer> iteration = initial.map(new MapFunction<Integer, Integer>() {
    @Override
    public Integer map(Integer i) throws Exception {
        double x = Math.random();
        double y = Math.random();
        // 小于1，i就加1
        return i + ((x * x + y * y < 1) ? 1 : 0);
    }
});

// Iteratively transform the IterativeDataSet 
// 按照迭代方式获取新的数据集
DataSet<Integer> count = initial.closeWith(iteration);

count.map(new MapFunction<Integer, Double>() {
    @Override
    public Double map(Integer count) throws Exception {
        return count / (double) 10000 * 4;
    }
}).print();

env.execute("Iterative Pi Example");
```

## Delta Iterations



# 在函数中操作数据对象

用户定义的转换函数运行时，flink传递给函数的对象称为**输入对象**，函数将结果返回给flink时的对象称为**输出对象**。

Flink有两种不同的模式来控制Flink运行时 创建和重用**输入对象**的方式 ，此行为也影响了用户函数如何处理**输入对象**和**输出对象**。

## Object-Reuse Disabled (DEFAULT)

禁用对象重用，默认的方式，此模式确保用户函数接收新的输入对象，此模式用起来更安全。

但是会带来一定的开销，以及更高的java GC 活动。

下表说明用户函数如何在禁用对象重用模式下访问输入和输出对象。

| Operation                    |                 Guarantees and Restrictions                  |
| :--------------------------- | :----------------------------------------------------------: |
| **Reading Input Objects**    | 方法调用时，**输入对象**的值不会更改，包含iterable提供的对象。例如，在iterable提供的List or Map中收集输入对象是安全的，但是在函数调用结束后，对象可以修改，所以在函数与函数间调用时，对象不安全。 |
| **Modifying Input Objects**  |                You may modify input objects.                 |
| **Emitting Input Objects**   | You may emit input objects. The value of an input object may have changed after it was emitted. It is **not safe** to read an input object after it was emitted. |
| **Reading Output Objects**   | An object that was given to a Collector or returned as method result might have changed its value. It is **not safe** to read an output object. |
| **Modifying Output Objects** | You may modify an object after it was emitted and emit it again. |

禁用对象重用（默认）模式的编码准则：

- 不要在方法调用之间记住和读取**输入对象**。

- 发射后不要读取对象。

## Object-Reuse Enabled

启用对象重用，Flink可以最大限度的最小化对象实例化的数量，这可以提高java GC的能力。

通过调用ExecutionConfig.enableObjectReuse（）激活启用对象重用模式。

| Operation                                                    |                 Guarantees and Restrictions                  |
| :----------------------------------------------------------- | :----------------------------------------------------------: |
| **Reading input objects received as regular method parameters** | 输入对象 received as regular method arguments are not modified within a function call. Objects may be modified after method call is left. It is **not safe** to remember objects across function calls. |
| **Reading input objects received from an Iterable parameter** | Input objects received from an Iterable are only valid until the next() method is called. An Iterable or Iterator may serve the same object instance multiple times. It is **not safe** to remember input objects received from an Iterable, e.g., by putting them in a List or Map. |
| **Modifying Input Objects**                                  | You **must not** modify input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). |
| **Emitting Input Objects**                                   | You **must not** emit input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). |
| **Reading Output Objects**                                   | An object that was given to a Collector or returned as method result might have changed its value. It is **not safe** to read an output object. |
| **Modifying Output Objects**                                 |      You may modify an output object and emit it again.      |

**Coding guidelines for object-reuse enabled:**

- Do not remember input objects received from an `Iterable`.
- Do not remember and read input objects across method calls.
- Do not modify or emit input objects, except for input objects of `MapFunction`, `FlatMapFunction`, `MapPartitionFunction`, `GroupReduceFunction`, `GroupCombineFunction`, `CoGroupFunction`, and `InputFormat.next(reuse)`.
- To reduce object instantiations, you can always emit a dedicated output object which is repeatedly modified but never read.

# Semantic Annotations 语义标注

# Broadcast Variables广播变量

把一个数据集提供给所有的并行实例。

广播：广播集通过withBroadcastSet（数据集，字符串）按名称注册

访问：可通过目标运算符处的getRuntimeContext().getBroadcastVariable（字符串）访问。

```java
// 1. The DataSet to be broadcast
DataSet<Integer> toBroadcast = env.fromElements(1, 2, 3);

DataSet<String> data = env.fromElements("a", "b");

data.map(new RichMapFunction<String, String>() {
    @Override
    public void open(Configuration parameters) throws Exception {
      // 3. Access the broadcast DataSet as a Collection
      Collection<Integer> broadcastSet = getRuntimeContext().getBroadcastVariable("broadcastSetName");
    }


    @Override
    public String map(String value) throws Exception {
        ...
    }
}).withBroadcastSet(toBroadcast, "broadcastSetName"); // 2. Broadcast the DataSet
```

注意：要广播的变量应该很小，实际应用中应该是维表。

# Distributed Cache 分布式缓存

