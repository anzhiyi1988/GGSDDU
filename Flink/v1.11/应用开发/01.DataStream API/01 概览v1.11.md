# Flink DataStream API 编程指南

DataStream programs 是 Flink实现 **流数据** 转换的常规程序(例如过滤、更新状态、定义窗口、聚合等），**流数据** 通过各种 `source` 被创建 (如消息队列、套接字流、文件等)，处理结果通过 `sink` 返回到文件或者标准输出(命令行终端)。

Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.

为了创建自己的flink程序，我们建议从 [anatomy of a Flink Program](https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/datastream_api.html#anatomy-of-a-flink-program) 开始，并逐步的增加自定义的**流转换** [stream transformations](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html). 其他章节作为附加操作和高级功能的参考。

## 什么是DataStream?

The DataStream API 的名称原子于一个特殊的 `DataStream` 类，该类用于表示Flink程序中的数据集合，You can think of them as immutable[不可变] collections of data that can contain duplicates，这些数据集合可以是有限的，也可以是无限的，但是处理他们的API是相同的。

`DataStream` 类似于java中的常规的 `Collection` 集合类，但是某些关键方法有很大不同，比如当一个`DataStream` 被建立后，他就不可更改，不能增加移动元素，也不能简单的查看里面的元素，而是通过 `DataStream` API ( `transformations`)进行操作。

在flink程序中个，通过 `source` 来创建初始的 `DataStream` ，然后可以通过诸如 `map`, `filter` 等操作来转换组合数据流，并得到一个新的流。

## 剖析一个Flink程序

Flink程序看起来像一个转换 `DataStreams` 的常规程序，程序的基本组成相同：

1. 获得一个 `execution environment` 
2. 加载或者创建初始数据
3. 在数据上指定 transformations 函数
4. 指定计算后的结果输出到哪里
5. 触发程序开始执行

现在对每个步骤进行简单介绍。

 `StreamExecutionEnvironment` 是Flink程序的基础，可以通过 `StreamExecutionEnvironment` 中的静态方法获取：

```java
getExecutionEnvironment()

createLocalEnvironment()

createRemoteEnvironment(String host, int port, String... jarFiles)
```

一把情况下通过 `getExecutionEnvironment()` 获取。

对于指定的 `source` ，执行环境有多种方法读取文件：如CSV文件逐行读取，或者指定其他的 `source` 。如果按照行序列读取文本，可用如下方式：

```java
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream<String> text = env.readTextFile("file:///path/to/file");
```

接下来可以用转换函数得到一个新的`DataStream` ，如下：

```java
DataStream<String> input = ...;

DataStream<Integer> parsed = input.map(new MapFunction<String, Integer>() {
    @Override
    public Integer map(String value) {
        return Integer.parseInt(value);
    }
});
```

在把String转成Intege后，可以得到一个新的 `DataStream` 。

当某一个已经包含了需要的最终结果，可以通过sink输出到外部系统中。例如：

```java
writeAsText(String path)

print()
```



当程序完成后，需要通过 `StreamExecutionEnvironment` 执行  `execute()` 方法来触发程序执行。根据`ExecutionEnvironment`的类别程序将在本地或者集群上执行。

`execute()` 将一直等待任务的完成，并返回  `JobExecutionResult`,  结果包含执行时间和累加器结果。

如果不想等待任务完成，可以触发异步执行 `executeAysnc()` ，他将返回一个 `JobClient` 用于观测任务。例如下面是通过 `executeAysnc()` 实现 `execute()` 的语法：

```java
final JobClient jobClient = env.executeAsync();

final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get();
```



理解何时以及如何执行Flink操作至关重要，所有的Flink程序都是懒执行：当执行main方法后，数据加载和转换不是立刻发生的，而是所有的操作被创建并添加到dataflow图中，所有操作是在触发`execute()`后才开始真正执行。

## Example Program

The following program is a complete, working example of streaming window word count application, that counts the words coming from a web socket in 5 second windows. You can copy & paste the code to run it locally.

```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

public class WindowWordCount {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Tuple2<String, Integer>> dataStream = env
                .socketTextStream("localhost", 9999)
                .flatMap(new Splitter())
                .keyBy(value -> value.f0)
                .timeWindow(Time.seconds(5))
                .sum(1);

        dataStream.print();

        env.execute("Window WordCount");
    }

    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {
        @Override
        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {
            for (String word: sentence.split(" ")) {
                out.collect(new Tuple2<String, Integer>(word, 1));
            }
        }
    }

}
```

To run the example program, start the input stream with netcat first from a terminal:

```
nc -lk 9999
```

Just type some words hitting return for a new word. These will be the input to the word count program. If you want to see counts greater than 1, type the same word again and again within 5 seconds (increase the window size from 5 seconds if you cannot type that fast ☺).

## Data Sources

`Source`是程序的输入，可以通过 `StreamExecutionEnvironment.addSource(sourceFunction)`方法增加一个`source` 。flink提供了很多预实现的`source` ，但是用户也可以编写自定义的source，

- 如果是 `non-parallel source` 需要实现 `SourceFunction` 接口，
- 如果是 `parallel source` 则需要实现 `ParallelSourceFunction` 接口或者继承 `RichParallelSourceFunction` 类。

以下是预定的source ：

File-based:

- `readTextFile(path)` - Reads text files, i.e. files that respect the `TextInputFormat` specification, line-by-line and returns them as Strings.

- `readFile(fileInputFormat, path)` - Reads (once) files as dictated by the specified file input format.

- `readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)` - This is the method called internally by the two previous ones. It reads files in the `path` based on the given `fileInputFormat`. Depending on the provided `watchType`, this source may periodically monitor (every `interval` ms) the path for new data (`FileProcessingMode.PROCESS_CONTINUOUSLY`), or process once the data currently in the path and exit (`FileProcessingMode.PROCESS_ONCE`). Using the `pathFilter`, the user can further exclude files from being processed.

  *IMPLEMENTATION:*

  Under the hood, Flink splits the file reading process into two sub-tasks, namely *directory monitoring* and *data reading*. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, **non-parallel** (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the `watchType`), find the files to be processed, divide them in *splits*, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.

  *IMPORTANT NOTES:*

  1. If the `watchType` is set to `FileProcessingMode.PROCESS_CONTINUOUSLY`, when a file is modified, its contents are re-processed entirely. This can break the “exactly-once” semantics, as appending data at the end of a file will lead to **all** its contents being re-processed.
  2. If the `watchType` is set to `FileProcessingMode.PROCESS_ONCE`, the source scans the path **once** and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.

Socket-based:

- `socketTextStream` - Reads from a socket. Elements can be separated by a delimiter.

Collection-based:

- `fromCollection(Collection)` - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.
- `fromCollection(Iterator, Class)` - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.
- `fromElements(T ...)` - Creates a data stream from the given sequence of objects. All objects must be of the same type.
- `fromParallelCollection(SplittableIterator, Class)` - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
- `generateSequence(from, to)` - Generates the sequence of numbers in the given interval, in parallel.

Custom:

- `addSource` - Attach a new source function. For example, to read from Apache Kafka you can use `addSource(new FlinkKafkaConsumer010<>(...))`. See [connectors](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/index.html) for more details.

## DataStream Transformations

Please see [operators](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html) for an overview of the available stream transformations.

## Data Sinks

`sink` 消费 `DataStreams` 并发送数据到文件、套接字、扩张系统或者直接打印。flink自带各种内置的输出：

- `writeAsText()` / `TextOutputFormat` - Writes elements line-wise as Strings. The Strings are obtained by calling the *toString()* method of each element.
- `writeAsCsv(...)` / `CsvOutputFormat` - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the *toString()* method of the objects.
- `print()` / `printToErr()` - Prints the *toString()* value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to *print*. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.
- `writeUsingOutputFormat()` / `FileOutputFormat` - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.
- `writeToSocket` - Writes elements to a socket according to a `SerializationSchema`
- `addSink` - 调用自定义的`sink` ， Flink comes bundled with connectors to other systems (such as Apache Kafka) that are implemented as sink functions.

`DataStream`的`write*()`方法主要用于调试时使用，这些方法没有增加Flink 的检查点`checkpointing`功能，这意味着这些方法至少有一次语义（**这句话直译的，不懂什么意思**）。数据刷新到目标系统依赖`OutputFormat`的实现，不是所有数据在发送到`OutputFormat`后都可以立即显示在目标系统中，而且在失败的情况下，数据可能会丢失。

想仅一次、可靠的传输数据到文件系统，需要使用`flink-connector-filesystem`，另外如果是通过`.addSink(...)`方法自定义实现，为了一次语义可以参与Flink检查点。

## Iterations

`Iterative streaming`是function的一个步骤，他返回`IterativeStream`，作为一个流式程序`iterations`没有最大数，所以需要指定那部分流数据反馈给`iteration` ，以及那部分数据通过 `split`或 `filter`转发到下游。

以下是一个filter的例子，先建立`IterativeStream`：

```java
IterativeStream<Integer> iteration = input.iterate();
```

接下来的逻辑是 使用一系列转换函数在循环内执行，下面是使用简单的`map`转换：

```java
DataStream<Integer> iterationBody = iteration.map(/* this is executed many times */);
```

关闭或者定义迭代尾部需要调用`IterativeStream`的`closeWith(feedbackStream)`方法，通过`closeWith(feedbackStream)`方法的数据流(即`feedbackStream`)将反馈给`iteration` 头，A common pattern is to use a filter to separate the part of the stream that is fed back, and the part of the stream which is propagated forward. These filters can, e.g., define the “termination” logic, where an element is allowed to propagate downstream rather than being fed back.

```java
iteration.closeWith(iterationBody.filter(/* one part of the stream */));
DataStream<Integer> output = iterationBody.filter(/* some other part of the stream */);
```

例如从一系列整数中连续减1，直到为0：

```java
DataStream<Long> someIntegers = env.generateSequence(0, 1000);

IterativeStream<Long> iteration = someIntegers.iterate();

DataStream<Long> minusOne = iteration.map(new MapFunction<Long, Long>() {
  @Override
  public Long map(Long value) throws Exception {
    return value - 1 ;
  }
});

DataStream<Long> stillGreaterThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value > 0);
  }
});

iteration.closeWith(stillGreaterThanZero);

DataStream<Long> lessThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value <= 0);
  }
});
```

## Execution Parameters

`StreamExecutionEnvironment` 通过 `ExecutionConfig` 可以给任务配置运行时的各种参数。

可以参考 [execution configuration](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/execution_configuration.html)获取跟多参数的解释，以下参数特别适用于 DataStream API:

- `setAutoWatermarkInterval(long milliseconds)`: 设置自动水印的发送间隔，可以通`long getAutoWatermarkInterval()`获取当前的值。

### 容错 Fault Tolerance

[State & Checkpointing](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/checkpointing.html) 介绍如何开启和设置 Flink的 checkpointing机制。

### 控制延迟 Controlling Latency

通常数据元素不是一个挨一个的在网络上传输，而是有缓冲的，缓冲区的大小可以通过flink的config文件设置。这个是一个提高吞吐的好方法，但也引起了延迟的问题。控制吞吐和延迟可以通过env.setBufferTimeout(timeoutMillis)在环境或者指定操作上设置，这个是延迟的最大时间，到达时间后，缓冲区无论是否填满都自动发送，默认值是100ms：

```java
LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
env.setBufferTimeout(timeoutMillis);

env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis);
```

设置为-1时，则只会在缓冲区填满后发送，如果想要低延迟，可以设置此值接近0 (for example 5 or 10 ms)，设置为0是不允许的，那会严重影响性能。

## Debugging

在程序部署到集群前，最好是测试一下算法是否是我们期望的那个，因此，分析程序通常是要经历多次检查结果、调试、改进。

flink提供了本地IDE的调试，本节介绍如何简化开发flink程序。

### Local Execution Environment

A `LocalStreamEnvironment` starts a Flink system within the same JVM process it was created in. 

如果从IDE启动 LocalEnvironment ，就可以设置设置断点并调试程序。

A LocalEnvironment is created and used as follows:

```java
final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

DataStream<String> lines = env.addSource(/* some source */);
// build your program

env.execute();
```

### Collection Data Sources

flink提供特殊的源来支持java的collection数据结构，这样便于测试，当程序开始测试时，source和sink能很容的被替换为读写外部系统的source和sink，使用如下：

Flink provides special data sources which are backed by Java collections to ease testing. Once a program has been tested, the sources and sinks can be easily replaced by sources and sinks that read from / write to external systems.

Collection data sources can be used as follows:

```java
final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

// Create a DataStream from a list of elements
DataStream<Integer> myInts = env.fromElements(1, 2, 3, 4, 5);

// Create a DataStream from any Java collection
List<Tuple2<String, Integer>> data = ...
DataStream<Tuple2<String, Integer>> myTuples = env.fromCollection(data);

// Create a DataStream from an Iterator
Iterator<Long> longIt = ...
DataStream<Long> myLongs = env.fromCollection(longIt, Long.class);
```

**Note:** 当前collection类型的 source 要求其收集的数据必须实现`Serializable`，而且collection类型的data sources不能并行执行。

### Iterator Data Sink

flink还提供了数据流的收集器，用于测试和调试，如下：

```java
import org.apache.flink.streaming.experimental.DataStreamUtils

DataStream<Tuple2<String, Integer>> myResult = ...
Iterator<Tuple2<String, Integer>> myOutput = DataStreamUtils.collect(myResult)
```

**Note:** `flink-streaming-contrib` module is removed from Flink 1.5.0. Its classes have been moved into `flink-streaming-java` and `flink-streaming-scala`.