https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/api_concepts.html

[TOC]

# Flink java项目搭建

执行mvn命令构建项目

```
mvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-java -DarchetypeVersion=1.10.0
```



# Flink编程

## 概述

Flink完成了分布式集合转换的规范化，只要我们使用它的api**正确的**编程，那么我们的计算就是分布式的。

Flink实现的计算过程包含很多：过滤、映射、更新状态、join、分组、定义窗口、聚合等。

针对有界和无界两种数据，Flink提供了DataSet API编写批处理、DataStream API编写流处理。

`DataSet` 和 `DataStream` 类表示程序中的数据，他们都是不可变的集合（和Map、List、Set同等对待，只不过这两个的计算方法内置了很多），通过诸如map、filter等方法对数据进行转换而派生新的集合。

Java DataSet API 的核心类包 org.apache.flink.api.java

Java DataStream API 的核心类包 org.apache.flink.streaming.api 

## 编程步骤

### 1.获取执行环境

`DataStream`  通过 `StreamExecutionEnvironment` 获取

```java
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
```

`DataSet` 通过 `ExecutionEnvironment` 获取

```java
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
```

### 2.加载/创建初始数据

数据的获取方式支持很多种：csv逐行读取、自定义输入格式等。如下，`DataStream`   读取文件

```java
DataStream<String> input = env.readTextFile(input);
```

### 3.指定对数据的转换操作

这里用到的急救室一系列的已经实现的处理函数，也可以自定义处理方式，比如下面是把字符转换成数字，得到一个新的集合

```java
DataStream<Integer> parsed = input.map(new MapFunction<String, Integer>() {
    @Override
    public Integer map(String value) {
        return Integer.parseInt(value);
    }
});
```

### 4.指定计算结果存放的位置

```java
writeAsText(String path)
print()
```

### 5.触发程序执行

```java
env.execute("Streaming work");
```



## 程序的延迟计算

无论在本地还是集群执行，所有的 Flink 程序都是延迟执行的：当程序的 main 方法被执行时，并不立即执行数据的加载和转换，而是创建每个操作并将其加入到程序的执行计划中。当执行环境调用 `execute()` 方法显式地触发执行的时候才真正执行各个操作。

延迟计算允许你构建复杂的程序，Flink 将其作为整体计划单元来执行。



## 分组操作

Flink有自己的数组类型，叫Tuple，以下是流数据处理实例：

```java
DataStream<Tuple3<Integer,String,Long>> input = // [...]
    
//group by first key
KeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0);

//group by first second key
KeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0,1);
```

如果是嵌套结构如下，则keyBy(0)将按照整个Tuple2作为分组键

```
DataStream<Tuple3<Tuple2<Integer, Float>,String,Long>> ds;
```

**使用字段表达式定义键**

那么数据结构如果是嵌套的，要如何分组呢，如下，如果是POJO类，曾需要传属性名字：

```java
// 普通的 POJO（简单的 Java 对象）
public class WC {
  public String word;
  public int count;
}
DataStream<WC> words = // [...]
DataStream<WC> wordCounts = words.keyBy("word").window(/*指定窗口*/);
```

**字段表达式语法**：

- 根据字段名称选择 POJO 的字段。例如 `“user”` 就是指 POJO 类型的“user”字段。
- 根据字段名称或 0 开始的字段索引选择 Tuple 的字段。例如 `“f0”` 和 `“5”` 分别指 Java Tuple 类型的第一个和第六个字段。
- 可以选择 POJO 和 Tuple 的嵌套字段。 例如，一个 POJO 类型有一个“user”字段还是一个 POJO 类型，那么 `“user.zip”` 即指这个“user”字段的“zip”字段。任意嵌套和混合的 POJO 和 Tuple都是支持的，例如 `“f1.user.zip”` 或 `“user.f3.1.zip”`。
- 可以使用 `"*"` 通配符表达式选择完整的类型。这也适用于非 Tuple 或 POJO 类型。

**字段表达式示例**:

```java
public static class WC {
  public ComplexNestedClass complex; //嵌套的 POJO
  private int count;
  // 私有字段（count）的 getter 和 setter
  public int getCount() {
    return count;
  }
  public void setCount(int c) {
    this.count = c;
  }
}
public static class ComplexNestedClass {
  public Integer someNumber;
  public float someFloat;
  public Tuple3<Long, Long, String> word;
  public IntWritable hadoopCitizen;
}
```

这些字段表达式对于以上代码示例都是合法的：

- `"count"`：`WC` 类的 count 字段。
- `"complex"`：递归选择 POJO 类型 `ComplexNestedClass` 的 complex 字段的全部字段。
- `"complex.word.f2"`：选择嵌套 `Tuple3` 类型的最后一个字段。
- `"complex.hadoopCitizen"`：选择 hadoop 的 `IntWritable` 类型。

## 自定义转换函数

**实现接口**

```java
class MyMapFunction implements MapFunction<String, Integer> {
  public Integer map(String value) { return Integer.parseInt(value); }
};
data.map(new MyMapFunction());
```

**匿名类**

```java
data.map(new MapFunction<String, Integer> () {
  public Integer map(String value) { return Integer.parseInt(value); }
});
```

**Lambda表达式**

```java
data.filter(s -> s.startsWith("http://"));
data.reduce((i1,i2) -> i1 + i2);
```

**富函数**



## 支持数据类型

有七种不同的数据类型：

### 1.Java Tuple 和 Scala Case Class

Tuple是复合类型

```java
DataStream<Tuple2<String, Integer>> wordCounts = env.fromElements(
    new Tuple2<String, Integer>("hello", 1),
    new Tuple2<String, Integer>("world", 2));

wordCounts.map(new MapFunction<Tuple2<String, Integer>, Integer>() {
    @Override
    public Integer map(Tuple2<String, Integer> value) throws Exception {
        return value.f1;
    }
});

wordCounts.keyBy(0); // .keyBy("f0") 也可以
```

### 2.Java POJO

**POJO类的规定**

- 类必须是公有的。
- 它必须有一个公有的无参构造器（默认构造器）。
- 所有的字段要么是公有的要么必须可以通过 getter 和 setter 函数访问。例如一个名为 `foo` 的字段，它的 getter 和 setter 方法必须命名为 `getFoo()` 和 `setFoo()`。
- 字段的类型必须被已注册的序列化程序所支持。

```java
public class WordWithCount {

    public String word;
    public int count;

    public WordWithCount() {}

    public WordWithCount(String word, int count) {
        this.word = word;
        this.count = count;
    }
}

DataStream<WordWithCount> wordCounts = env.fromElements(
    new WordWithCount("hello", 1),
    new WordWithCount("world", 2));

wordCounts.keyBy("word"); // 以字段表达式“word”为键
```

### 3.基本数据类型

`Integer`、 `String`、和 `Double`。

### 4.常规的类

### 5.值

### 6.Hadoop Writable

### 7.特殊类型